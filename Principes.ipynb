{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est une méthode d'apprentissage supervisé non paramétrique utilisé pour la classification et la régression. Le but est de créer un modeèle qui prédit la valeur de la variable 'target' par des décisions simples déduites des features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avantages\n",
    "\n",
    "- simple à comprendre et à interpréter. L'arbre peut être visualisé\n",
    "- requière peu de préparation de la data. Les autres techniques requièrent souvent une normalisation des données, la créations de valeurs factices et la suppression de valeurs vides (NaN).\n",
    "- peut gérer les problèmes de multi-output\n",
    "- posible de valider un modèle utilisant un des tests statistiques. Cela permet de se rendre compte de la fiabilité du modèle\n",
    "- fonctionne bien même si les hypothèses sont enfreintes par le modèle de base à partir duquel les données sont générées\n",
    "\n",
    "## Inconvénients\n",
    "\n",
    "- l'apprentissage peut créer des arbres biaisés si certaines classes dominent. Il est donc recommandé d'équilibrer le dataset avant de le .fit avec les decision tree\n",
    "- certains concepts sont difficiles à apprendre car les decision tree ne les espriment pas facilement, comme les problèmes de XOR, de parité ou de MUX\n",
    "- les valeurs apprenantes des decision tree peuvent créer des arbres trop complexes qui ne généralisent pas bien les données. c'est ce qu'on appelle l'overfitting. Des mécanismes comme le pruning, la définition du nombre minimum d'échantillons requis au niveau d'un noeud ou de la définition de la profondeur maximale de l'arbre sont nécessaires pour éviter ce problème "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Decision Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elles font partie des techniques d'apprentissage automatique. Cet algorithme combine les concepts de sous-espaces aléatoires et de bagging. L'algorithme RDF effectue un apprentissage sur de multiples decision tree entraînés sur des sous-ensembles de données légèrement différents. \n",
    "\n",
    "La base du calcul repose sur l'apprentissage par arbre de décision. La proposition de Breiman vise à corriger plusieurs inconvénients connus de la méthode initiale, comme la sensibilité des arbres uniques à l'ordre des prédicteurs, en calculant un ensemble de B arbres partiellement indépendants. \n",
    "\n",
    "Le principal inconvénient de cette méthode est que l'on perd l'aspect visuel des arbres de décision uniques. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
